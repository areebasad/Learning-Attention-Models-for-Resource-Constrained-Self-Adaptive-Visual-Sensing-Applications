{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd522d7d-663a-4ae7-97c7-73f135b9f851",
   "metadata": {},
   "source": [
    "## Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce7e42-6ce8-4626-beb8-879a420663ed",
   "metadata": {},
   "source": [
    "This notebook contains parts to analyse errors of detections performed with a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e2a2b5-da76-49d9-b3a9-9e891c46adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, ceil, floor\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8297db-d176-4479-8d98-79a8623620dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd6513-b319-4d7c-a5c5-f46a3b778134",
   "metadata": {},
   "source": [
    "#### 0- Function to calculate errors: Mean Absolute Error, Mean Squarred error, Difference, and Absolute difference between two series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d98d06b-3f62-4dbe-affe-9e620a68901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df_ground : pd.DataFrame, df_calculated : pd.DataFrame, estimated_column_name: str):\n",
    "    \n",
    "    true_values = df_ground      \n",
    "    calculated_values = df_calculated\n",
    "\n",
    "    true_values = true_values.set_index('timestamp')\n",
    "    calculated_values = calculated_values.set_index('timestamp')\n",
    "\n",
    "    true_values = true_values.sort_index()\n",
    "    calculated_values = calculated_values.sort_index()\n",
    "\n",
    "    # Visualize data\n",
    "    #plt.figure(figsize=(20, 10), dpi=80)\n",
    "    #plt.scatter(true_values.index, true_values[\"count\"], color=\"blue\", label=\"original\")\n",
    "    #plt.plot(calculated_values.index, calculated_values[estimated_column_name], color=\"red\", label=\"predicted\")\n",
    "    #plt.title(\"True Vs Calculated\")\n",
    "    #plt.legend()\n",
    "    #plt.show() \n",
    "    \n",
    "    #ic(true_values[\"count\"])\n",
    "    #ic(calculated_values[estimated_column_name])\n",
    "\n",
    "    \n",
    "    ae_all = abs(true_values[\"count\"] - calculated_values[estimated_column_name])\n",
    "    difference_all = true_values[\"count\"] - calculated_values[estimated_column_name]\n",
    "    mae = mean_absolute_error(true_values, calculated_values)\n",
    "    mse = mean_squared_error(true_values, calculated_values)\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"all_absolute-errors\": ae_all,\n",
    "        \"all_difference\": difference_all\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5cacd85b-6e99-4148-88de-e1daeb8a3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(part, whole):\n",
    "    percentage = 100 * float(part)/float(whole)\n",
    "    return float(percentage) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7df692-65fc-4d6f-883b-79420d3de8be",
   "metadata": {},
   "source": [
    "### Error Analysis and Generate Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e014362-076c-49c5-b9a0-3e87ea6f1f5b",
   "metadata": {},
   "source": [
    "#### 1- Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8457359-0834-4d17-8782-03f8257729ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cam = jervskogen_1 and Percentage = 100\n",
      "Total person detected Ground:  4202\n",
      "Random\n",
      "Total person detected Random with errors:  4202\n",
      "Total Samples: 13597\n",
      "Underestimated count: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives count: 0 and Percentage: 0.0 \n",
      "Correct count: 13597 and Percentage: 100.0 \n",
      "Underestimated sum: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives sum: 0 and Percentage: 0.0 \n",
      "Correct sum: 4202 and Percentage: 100.0 \n",
      "Learned\n",
      "Total person detected Learned with errors:  4202\n",
      "Difference b/w Ground and Learned:  0\n",
      "Learned missed percentage:  0.0\n",
      "\n",
      "Cam = jervskogen_1 and Percentage = 80\n",
      "Total person detected Ground:  4202\n",
      "Random\n",
      "Total person detected Random with errors:  3015\n",
      "Total Samples: 13597\n",
      "Underestimated count: 1164 and Percentage: 8.560711921747444 \n",
      "Overestimated/False positives count: 208 and Percentage: 1.529749209384423 \n",
      "Correct count: 12225 and Percentage: 89.90953886886814 \n",
      "Underestimated sum: 1434 and Percentage: 34.12660637791528 \n",
      "Overestimated/False positives sum: 247 and Percentage: 5.878153260352213 \n",
      "Correct sum: 2768 and Percentage: 65.87339362208472 \n",
      "Learned\n",
      "Total person detected Learned with errors:  4036\n",
      "Difference b/w Ground and Learned:  166\n",
      "Learned missed percentage:  3.9504997620180866\n",
      "\n",
      "Cam = jervskogen_1 and Percentage = 60\n",
      "Total person detected Ground:  4202\n",
      "Random\n",
      "Total person detected Random with errors:  2224\n",
      "Total Samples: 13597\n",
      "Underestimated count: 1585 and Percentage: 11.656983158049568 \n",
      "Overestimated/False positives count: 258 and Percentage: 1.897477384717217 \n",
      "Correct count: 11754 and Percentage: 86.44553945723321 \n",
      "Underestimated sum: 2272 and Percentage: 54.06949071870538 \n",
      "Overestimated/False positives sum: 294 and Percentage: 6.996668253212755 \n",
      "Correct sum: 1930 and Percentage: 45.930509281294626 \n",
      "Learned\n",
      "Total person detected Learned with errors:  3880\n",
      "Difference b/w Ground and Learned:  322\n",
      "Learned missed percentage:  7.66301761066159\n",
      "\n",
      "Cam = jervskogen_1 and Percentage = 40\n",
      "Total person detected Ground:  4202\n",
      "Random\n",
      "Total person detected Random with errors:  1462\n",
      "Total Samples: 13597\n",
      "Underestimated count: 1848 and Percentage: 13.591233360300068 \n",
      "Overestimated/False positives count: 243 and Percentage: 1.7871589321173789 \n",
      "Correct count: 11506 and Percentage: 84.62160770758256 \n",
      "Underestimated sum: 3007 and Percentage: 71.56116135173727 \n",
      "Overestimated/False positives sum: 267 and Percentage: 6.35411708710138 \n",
      "Correct sum: 1195 and Percentage: 28.43883864826273 \n",
      "Learned\n",
      "Total person detected Learned with errors:  3379\n",
      "Difference b/w Ground and Learned:  823\n",
      "Learned missed percentage:  19.585911470728224\n",
      "\n",
      "Cam = jervskogen_1 and Percentage = 20\n",
      "Total person detected Ground:  4202\n",
      "Random\n",
      "Total person detected Random with errors:  580\n",
      "Total Samples: 13597\n",
      "Underestimated count: 2072 and Percentage: 15.238655585790983 \n",
      "Overestimated/False positives count: 79 and Percentage: 0.5810105170258145 \n",
      "Correct count: 11446 and Percentage: 84.1803338971832 \n",
      "Underestimated sum: 3707 and Percentage: 88.21989528795811 \n",
      "Overestimated/False positives sum: 85 and Percentage: 2.02284626368396 \n",
      "Correct sum: 495 and Percentage: 11.780104712041885 \n",
      "Learned\n",
      "Total person detected Learned with errors:  2168\n",
      "Difference b/w Ground and Learned:  2034\n",
      "Learned missed percentage:  48.40552118039029\n",
      "\n",
      "Cam = jervskogen_2 and Percentage = 100\n",
      "Total person detected Ground:  3077\n",
      "Random\n",
      "Total person detected Random with errors:  3077\n",
      "Total Samples: 13597\n",
      "Underestimated count: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives count: 0 and Percentage: 0.0 \n",
      "Correct count: 13597 and Percentage: 100.0 \n",
      "Underestimated sum: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives sum: 0 and Percentage: 0.0 \n",
      "Correct sum: 3077 and Percentage: 100.0 \n",
      "Learned\n",
      "Total person detected Learned with errors:  3077\n",
      "Difference b/w Ground and Learned:  0\n",
      "Learned missed percentage:  0.0\n",
      "\n",
      "Cam = jervskogen_2 and Percentage = 80\n",
      "Total person detected Ground:  3077\n",
      "Random\n",
      "Total person detected Random with errors:  2384\n",
      "Total Samples: 13597\n",
      "Underestimated count: 490 and Percentage: 3.603736118261381 \n",
      "Overestimated/False positives count: 83 and Percentage: 0.6104287710524381 \n",
      "Correct count: 13024 and Percentage: 95.78583511068618 \n",
      "Underestimated sum: 793 and Percentage: 25.77185570360741 \n",
      "Overestimated/False positives sum: 100 and Percentage: 3.249918752031199 \n",
      "Correct sum: 2284 and Percentage: 74.2281442963926 \n",
      "Learned\n",
      "Total person detected Learned with errors:  2927\n",
      "Difference b/w Ground and Learned:  150\n",
      "Learned missed percentage:  4.874878128046799\n",
      "\n",
      "Cam = jervskogen_2 and Percentage = 60\n",
      "Total person detected Ground:  3077\n",
      "Random\n",
      "Total person detected Random with errors:  1768\n",
      "Total Samples: 13597\n",
      "Underestimated count: 741 and Percentage: 5.449731558432007 \n",
      "Overestimated/False positives count: 95 and Percentage: 0.6986835331323086 \n",
      "Correct count: 12761 and Percentage: 93.85158490843568 \n",
      "Underestimated sum: 1418 and Percentage: 46.08384790380241 \n",
      "Overestimated/False positives sum: 109 and Percentage: 3.542411439714007 \n",
      "Correct sum: 1659 and Percentage: 53.9161520961976 \n",
      "Learned\n",
      "Total person detected Learned with errors:  2668\n",
      "Difference b/w Ground and Learned:  409\n",
      "Learned missed percentage:  13.292167695807604\n",
      "\n",
      "Cam = jervskogen_2 and Percentage = 40\n",
      "Total person detected Ground:  3077\n",
      "Random\n",
      "Total person detected Random with errors:  1231\n",
      "Total Samples: 13597\n",
      "Underestimated count: 903 and Percentage: 6.64117084651026 \n",
      "Overestimated/False positives count: 85 and Percentage: 0.6251378980657498 \n",
      "Correct count: 12609 and Percentage: 92.73369125542399 \n",
      "Underestimated sum: 1940 and Percentage: 63.04842378940526 \n",
      "Overestimated/False positives sum: 94 and Percentage: 3.054923626909327 \n",
      "Correct sum: 1137 and Percentage: 36.95157621059473 \n",
      "Learned\n",
      "Total person detected Learned with errors:  2444\n",
      "Difference b/w Ground and Learned:  633\n",
      "Learned missed percentage:  20.57198570035749\n",
      "\n",
      "Cam = jervskogen_2 and Percentage = 20\n",
      "Total person detected Ground:  3077\n",
      "Random\n",
      "Total person detected Random with errors:  565\n",
      "Total Samples: 13597\n",
      "Underestimated count: 1033 and Percentage: 7.597264102375524 \n",
      "Overestimated/False positives count: 53 and Percentage: 0.38979186585276165 \n",
      "Correct count: 12511 and Percentage: 92.01294403177171 \n",
      "Underestimated sum: 2576 and Percentage: 83.7179070523237 \n",
      "Overestimated/False positives sum: 64 and Percentage: 2.0799480012999676 \n",
      "Correct sum: 501 and Percentage: 16.282092947676308 \n",
      "Learned\n",
      "Total person detected Learned with errors:  1825\n",
      "Difference b/w Ground and Learned:  1252\n",
      "Learned missed percentage:  40.68898277543062\n",
      "\n",
      "Cam = nilsbyen_2 and Percentage = 100\n",
      "Total person detected Ground:  3264\n",
      "Random\n",
      "Total person detected Random with errors:  3264\n",
      "Total Samples: 13592\n",
      "Underestimated count: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives count: 0 and Percentage: 0.0 \n",
      "Correct count: 13592 and Percentage: 100.0 \n",
      "Underestimated sum: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives sum: 0 and Percentage: 0.0 \n",
      "Correct sum: 3264 and Percentage: 100.0 \n",
      "Learned\n",
      "Total person detected Learned with errors:  3264\n",
      "Difference b/w Ground and Learned:  0\n",
      "Learned missed percentage:  0.0\n",
      "\n",
      "Cam = nilsbyen_2 and Percentage = 80\n",
      "Total person detected Ground:  3264\n",
      "Random\n",
      "Total person detected Random with errors:  2657\n",
      "Total Samples: 13592\n",
      "Underestimated count: 585 and Percentage: 4.304002354326074 \n",
      "Overestimated/False positives count: 226 and Percentage: 1.662742789876398 \n",
      "Correct count: 12781 and Percentage: 94.03325485579754 \n",
      "Underestimated sum: 851 and Percentage: 26.07230392156863 \n",
      "Overestimated/False positives sum: 244 and Percentage: 7.4754901960784315 \n",
      "Correct sum: 2413 and Percentage: 73.92769607843137 \n",
      "Learned\n",
      "Total person detected Learned with errors:  3168\n",
      "Difference b/w Ground and Learned:  96\n",
      "Learned missed percentage:  2.9411764705882355\n",
      "\n",
      "Cam = nilsbyen_2 and Percentage = 60\n",
      "Total person detected Ground:  3264\n",
      "Random\n",
      "Total person detected Random with errors:  1983\n",
      "Total Samples: 13592\n",
      "Underestimated count: 862 and Percentage: 6.341965862271924 \n",
      "Overestimated/False positives count: 215 and Percentage: 1.5818128310771042 \n",
      "Correct count: 12515 and Percentage: 92.07622130665098 \n",
      "Underestimated sum: 1512 and Percentage: 46.32352941176471 \n",
      "Overestimated/False positives sum: 231 and Percentage: 7.077205882352941 \n",
      "Correct sum: 1752 and Percentage: 53.67647058823529 \n",
      "Learned\n",
      "Total person detected Learned with errors:  2828\n",
      "Difference b/w Ground and Learned:  436\n",
      "Learned missed percentage:  13.357843137254902\n",
      "\n",
      "Cam = nilsbyen_2 and Percentage = 40\n",
      "Total person detected Ground:  3264\n",
      "Random\n",
      "Total person detected Random with errors:  1331\n",
      "Total Samples: 13592\n",
      "Underestimated count: 1048 and Percentage: 7.710417892878163 \n",
      "Overestimated/False positives count: 141 and Percentage: 1.0373749264273102 \n",
      "Correct count: 12403 and Percentage: 91.25220718069453 \n",
      "Underestimated sum: 2077 and Percentage: 63.63357843137255 \n",
      "Overestimated/False positives sum: 144 and Percentage: 4.411764705882353 \n",
      "Correct sum: 1187 and Percentage: 36.36642156862745 \n",
      "Learned\n",
      "Total person detected Learned with errors:  2442\n",
      "Difference b/w Ground and Learned:  822\n",
      "Learned missed percentage:  25.183823529411764\n",
      "\n",
      "Cam = nilsbyen_2 and Percentage = 20\n",
      "Total person detected Ground:  3264\n",
      "Random\n",
      "Total person detected Random with errors:  657\n",
      "Total Samples: 13592\n",
      "Underestimated count: 1169 and Percentage: 8.600647439670395 \n",
      "Overestimated/False positives count: 74 and Percentage: 0.544437904649794 \n",
      "Correct count: 12349 and Percentage: 90.8549146556798 \n",
      "Underestimated sum: 2684 and Percentage: 82.23039215686273 \n",
      "Overestimated/False positives sum: 77 and Percentage: 2.3590686274509802 \n",
      "Correct sum: 580 and Percentage: 17.769607843137255 \n",
      "Learned\n",
      "Total person detected Learned with errors:  1556\n",
      "Difference b/w Ground and Learned:  1708\n",
      "Learned missed percentage:  52.32843137254902\n",
      "\n",
      "Cam = nilsbyen_3 and Percentage = 100\n",
      "Total person detected Ground:  6770\n",
      "Random\n",
      "Total person detected Random with errors:  6770\n",
      "Total Samples: 13590\n",
      "Underestimated count: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives count: 0 and Percentage: 0.0 \n",
      "Correct count: 13590 and Percentage: 100.0 \n",
      "Underestimated sum: 0 and Percentage: 0.0 \n",
      "Overestimated/False positives sum: 0 and Percentage: 0.0 \n",
      "Correct sum: 6770 and Percentage: 100.0 \n",
      "Learned\n",
      "Total person detected Learned with errors:  6770\n",
      "Difference b/w Ground and Learned:  0\n",
      "Learned missed percentage:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/bknnh1gx3m17c962w063krcc0000gn/T/ipykernel_23521/3456056830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# Read from csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mdata_policy_learned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolicy_percentage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/datasets/local/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath_string_timestamps_learned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Select samples from required period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/yolo_env/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/yolo_env/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/yolo_env/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/yolo_env/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/yolo_env/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Code for plots of all policy percentages and cameras.\n",
    "import seaborn as sn\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "from pycm import *\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Parameters\n",
    "\n",
    "cams = ['jervskogen_1', 'jervskogen_2', 'nilsbyen_2', 'nilsbyen_3', 'skistua']\n",
    "policy_percentages = [ 100, 80, 60, 40, 20]\n",
    "image_num_grid_tiles = 64\n",
    "MONTHS = ['_2021-12', '_2022-01', '_2022-02', '_2022-03']     # 2022-01-11       # For all images it could be empty with underscore '_' or for month '_2022-01' \n",
    "all_months = '' # To store months\n",
    "number_of_months_to_include = 4\n",
    "DRAW_SUBFIGURES = False\n",
    "\n",
    "# Set dates\n",
    "start_date = '2021-12-01' # This is included, '2021-12-01'\n",
    "end_date = '2022-04-01'   # This is excluded, 2022-03-01'\n",
    "\n",
    "\n",
    "# Set file paths \n",
    "database_path = '../data/datasets/'    \n",
    "exp_name_folder_random = 'Experiment_operational_random/'\n",
    "exp_name_folder_learned = 'Experiment_operational_egreedy_with_alpha_0.5/'\n",
    "\n",
    "# Add all months and make a string\n",
    "for i in range(number_of_months_to_include):  \n",
    "    all_months +=  str(MONTHS[i])\n",
    "\n",
    "# Initialisation\n",
    "data_ground = dict() # To store df of ground data\n",
    "data_policy_random = dict() # To store df of policy\n",
    "data_policy_learned = dict() # To store df of policy\n",
    "data_policy_temp_random = dict()\n",
    "data_policy_temp_learned = dict()\n",
    "\n",
    "# For errors\n",
    "mean_dae_cams_policy_random = dict() # To store mean of daily average absolute error for all policies of all cameras \n",
    "\n",
    "\n",
    "\n",
    "mean_dae_cams_policy_learned = dict() # To store mean of daily average absolute error for all policies of all cameras \n",
    "\n",
    "\n",
    "\n",
    "# Iterate over all cameras/devices\n",
    "for cam in cams:\n",
    "    data_ground[cam] = pd.read_csv(database_path + 'local/all/'+ cam + '_all_timestamps_count' + '.csv', parse_dates=['timestamp'])\n",
    "    data_ground[cam] = data_ground[cam][(data_ground[cam]['timestamp'] >= start_date) & (data_ground[cam]['timestamp'] < end_date)]\n",
    "    data_ground[cam] = data_ground[cam][['timestamp', 'count']]\n",
    "    \n",
    "    total_person_detected_ground = data_ground[cam][\"count\"].sum()\n",
    "     #ic(data_ground[cam].describe())\n",
    "   \n",
    "    # For error Random\n",
    "    mae_policy_random = dict()\n",
    "    mse_policy_random = dict()\n",
    "    dae_policy_random = dict()\n",
    "    \n",
    "    # For error Learned\n",
    "    mae_policy_learned = dict()\n",
    "    mse_policy_learned = dict()\n",
    "    dae_policy_learned = dict()\n",
    "    \n",
    "    # Iterate over all percentages (100, 80, 60, 40, 20)\n",
    "    for policy_percentage in policy_percentages:\n",
    "            \n",
    "            ## Random\n",
    "            # Make path string to get read from policy csv file\n",
    "            path_string_timestamps_random = ''\n",
    "            path_string_timestamps_random = exp_name_folder_random + '/' + cam + '_all_timestamps_count_' + '_policy_' + str(policy_percentage) + '_tiles_' + str(image_num_grid_tiles) +  \"_MONTHS\" + all_months\n",
    "            \n",
    "            # Read from csv file\n",
    "            data_policy_random[policy_percentage] = pd.read_csv('../data/datasets/local/' + path_string_timestamps_random + '.csv', parse_dates=['timestamp'])   \n",
    "            \n",
    "            # Select samples from required period\n",
    "            data_policy_random[policy_percentage] = data_policy_random[policy_percentage][(data_policy_random[policy_percentage]['timestamp'] >= start_date ) & (data_policy_random[policy_percentage]['timestamp'] < end_date)]\n",
    "            \n",
    "            # Count total persons detected via policy\n",
    "            total_person_detected_policy_random = data_policy_random[policy_percentage][\"count\"].sum()\n",
    "            \n",
    "            policy_column_name = \"count\"\n",
    "            data_policy_temp_random[policy_percentage] = data_policy_random[policy_percentage][['timestamp',  policy_column_name]]\n",
    "            df_random = data_policy_temp_random[policy_percentage]\n",
    "            df_random_difference = data_ground[cam][\"count\"] - df_random[\"count\"]\n",
    "            #ic(df_random_difference)\n",
    "            \n",
    "                        #---------------------------------------------------#\n",
    "\n",
    "            \n",
    "            ## Learned\n",
    "            # Make path string to get read from policy csv file\n",
    "            path_string_timestamps_learned = ''\n",
    "            path_string_timestamps_learned = exp_name_folder_learned + '/' + cam + '_all_timestamps_count_' + '_policy_' + str(policy_percentage) + '_tiles_' + str(image_num_grid_tiles) +  \"_MONTHS\" + all_months\n",
    "            \n",
    "            # Read from csv file\n",
    "            data_policy_learned[policy_percentage] = pd.read_csv('../data/datasets/local/' + path_string_timestamps_learned + '.csv', parse_dates=['timestamp'])   \n",
    "            \n",
    "            # Select samples from required period\n",
    "            data_policy_learned[policy_percentage] = data_policy_learned[policy_percentage][(data_policy_learned[policy_percentage]['timestamp'] >= start_date ) & (data_policy_learned[policy_percentage]['timestamp'] < end_date)]\n",
    "            \n",
    "            # Count total persons detected via policy\n",
    "            total_person_detected_policy_learned = data_policy_learned[policy_percentage][\"count\"].sum()\n",
    "            \n",
    "            policy_column_name = \"count\"\n",
    "            data_policy_temp_learned[policy_percentage] = data_policy_learned[policy_percentage][['timestamp',  policy_column_name]]\n",
    "            df_learned = data_policy_temp_learned[policy_percentage]\n",
    "            df_learned_difference = data_ground[cam][\"count\"] - df_learned[\"count\"]\n",
    "\n",
    "            \n",
    "            #########################-------------------------#############################\n",
    "            # Here we calculate the difference of person detected from all samples\n",
    "            print (\"\\nCam = {} and Percentage = {}\".format(cam, policy_percentage))\n",
    "            print (\"Total person detected Ground: \", total_person_detected_ground)\n",
    "            \n",
    "            # Random\n",
    "            \n",
    "            tp_random_missed = total_person_detected_ground - total_person_detected_policy_random\n",
    "            #print (\"Difference b/w Ground and Random: \", (tp_random_missed))\n",
    "            tp_random_missed_percent =  percentage(tp_random_missed, total_person_detected_ground)\n",
    "            #print (\"Random missed percentage: \", (tp_random_missed_percent))\n",
    "            \n",
    "            # Count Samples\n",
    "            print (\"Random\\nTotal person detected Random with errors: \" , total_person_detected_policy_random)\n",
    "            random_total_samples = df_random[\"count\"].count()\n",
    "            print (\"Total Samples: {}\".format(random_total_samples))\n",
    "\n",
    "            #--With Samples Count--#\n",
    "            # Underestimated\n",
    "            random_indices_underestimated = np.where(df_random_difference > 0)\n",
    "            random_underestimated_person_count = df_random_difference[random_indices_underestimated[0]].count()\n",
    "            print (\"Count/nUnderestimated count: {} and Percentage: {} \".format(random_underestimated_person_count,(random_underestimated_person_count/ random_total_samples)*100))\n",
    "            \n",
    "            # Overestimated/False Positives\n",
    "            random_indices_overestimated = np.where(df_random_difference < 0)\n",
    "            random_overestimated_person_count = abs(df_random_difference[random_indices_overestimated[0]].count())\n",
    "            print (\"Overestimated/False positives count: {} and Percentage: {} \".format(random_overestimated_person_count,(random_overestimated_person_count/ random_total_samples)*100))\n",
    "            \n",
    "            # Correct\n",
    "            random_indices_correct = np.where(df_random_difference == 0)\n",
    "            random_correct_person_count = abs(df_random_difference[random_indices_correct[0]].count())\n",
    "            print (\"Correct count: {} and Percentage: {} \".format(random_correct_person_count,(random_correct_person_count/ random_total_samples)*100))\n",
    "            \n",
    "            \n",
    "             #--With Person Sum--#\n",
    "            # Underestimated\n",
    "            random_indices_underestimated = np.where(df_random_difference > 0)\n",
    "            random_underestimated_person_sum = df_random_difference[random_indices_underestimated[0]].sum()\n",
    "            print (\"Sum/nUnderestimated sum: {} and Percentage: {} \".format(random_underestimated_person_sum,(random_underestimated_person_sum/ total_person_detected_ground)*100))\n",
    "            \n",
    "            # Overestimated/False Positives\n",
    "            random_indices_overestimated = np.where(df_random_difference < 0)\n",
    "            random_overestimated_person_sum = abs(df_random_difference[random_indices_overestimated[0]].sum())\n",
    "            print (\"Overestimated/False positives sum: {} and Percentage: {} \".format(random_overestimated_person_sum,(random_overestimated_person_sum/ total_person_detected_ground)*100))\n",
    "            \n",
    "            # Correct\n",
    "            random_correct_detections = total_person_detected_policy_random - random_overestimated_person_sum\n",
    "            print (\"Correct sum: {} and Percentage: {} \".format(random_correct_detections,(random_correct_detections/ total_person_detected_ground)*100))\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            #random_correct_detections = total_person_detected_policy_random - random_overestimated_person_count\n",
    "            #print (\"Correct detections: {} and Percentage: {}\".format(random_correct_detections, (random_correct_detections/total_person_detected_ground)*100))\n",
    "            \n",
    "            \n",
    "            # To store value and later make plot\n",
    "            dae_policy_random[policy_percentage] = tp_random_missed_percent\n",
    "\n",
    "                        #---------------------------------------------------#\n",
    "            \n",
    "            # Learned\n",
    "            print (\"Learned\\nTotal person detected Learned with errors: \", total_person_detected_policy_learned)\n",
    "            tp_learned_missed = total_person_detected_ground - total_person_detected_policy_learned            \n",
    "            print (\"Difference b/w Ground and Learned: \", (total_person_detected_ground - total_person_detected_policy_learned))\n",
    "            tp_learned_missed_percent =  percentage(tp_learned_missed, total_person_detected_ground)\n",
    "            print (\"Learned missed percentage: \", (tp_learned_missed_percent))\n",
    "            \n",
    "            # To store value and later make plot\n",
    "            dae_policy_learned[policy_percentage] = tp_learned_missed_percent\n",
    "            \n",
    "            # Difference random with learned\n",
    "            #difference_random_learned = tp_random_missed - tp_learned_missed\n",
    "            #difference_random_learned_percentage = percentage(difference_random_learned, tp_random_missed)\n",
    "            #print(\"Missed difference random - learned \", difference_random_learned)\n",
    "            #print(\"Missed Percentage difference random - learned \", difference_random_learned_percentage)\n",
    "\n",
    "            \n",
    "            \n",
    "                        \n",
    "            #########################-------------------------#############################\n",
    "            # From here onwards we compare the ground truth with the policies (random and learned)\n",
    "            \n",
    "            # We consider samples where we detect the persons in the ground truth\n",
    "            df_ground_wo = data_ground[cam].loc[data_ground[cam]['count'] != 0]\n",
    "            \n",
    "            \n",
    "            # Then, we get only those timestamps from random policy array\n",
    "            df_random_wo = df_random[df_random['timestamp'].isin(df_ground_wo[\"timestamp\"].tolist())]\n",
    "            df_difference_error_random =  df_ground_wo[\"count\"] -  df_random_wo[\"count\"]\n",
    "            #ic(df_difference_error_random.mean())\n",
    "            # Store result mean of error difference\n",
    "            #dae_policy_random[policy_percentage] = df_difference_error_random.mean()\n",
    "            #  Store result cohen kappa without zeros\n",
    "            #dae_policy_random[policy_percentage] = cohen_kappa_score(df_ground_wo[\"count\"], df_random_wo[\"count\"])\n",
    "            #  Store result cohen kappa without zeros\n",
    "            #dae_policy_random[policy_percentage] = cohen_kappa_score(data_ground[cam][\"count\"], df_random[\"count\"])\n",
    "            \n",
    "            # Next, we get only those timestamps from learned policy array\n",
    "            df_learned_wo = df_learned[df_learned['timestamp'].isin(df_ground_wo[\"timestamp\"].tolist())]\n",
    "            df_difference_error_learned =  df_ground_wo[\"count\"] -  df_learned_wo[\"count\"]\n",
    "            #ic(df_difference_error_learned.mean())\n",
    "            ## Store result\n",
    "            #dae_policy_learned[policy_percentage] = df_difference_error_learned.mean()\n",
    "            ##  Store result cohen kappa without zeros\n",
    "           # dae_policy_learned[policy_percentage] = cohen_kappa_score(df_ground_wo[\"count\"], df_learned_wo[\"count\"])\n",
    "            ##  Store result cohen kappa with zeros\n",
    "            #dae_policy_learned[policy_percentage] = cohen_kappa_score(data_ground[cam][\"count\"], df_learned[\"count\"])\n",
    "            \n",
    "            #########################-------------------------#############################\n",
    "\n",
    "            # Calculate MAE and MSE of policy with respect to ground data\n",
    "            #results = calculate_metrics(data_ground[cam], data_policy_temp[policy_percentage], policy_column_name )\n",
    "            #mae_policy[policy_percentage] = results[\"mae\"]\n",
    "            #mse_policy[policy_percentage] = results[\"mse\"]\n",
    "            \n",
    "            ## Next calculate other statistics\n",
    "            # Confusion Matrix\n",
    "            #confusion_matrix = pd.crosstab(data_ground[cam][policy_column_name], data_policy_temp[policy_percentage][policy_column_name], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "            #sn.heatmap(confusion_matrix, annot=True)\n",
    "            #plt.show()\n",
    "            \n",
    "            #print(classification_report(data_ground[cam][policy_column_name], data_policy_temp[policy_percentage][policy_column_name]))\n",
    "            #print(cam + \"_\" + str(policy_percentage))\n",
    "            #print(cohen_kappa_score(data_ground[cam][policy_column_name], data_policy_temp[policy_percentage][policy_column_name]))\n",
    "\n",
    "            #Confusion_Matrix = ConfusionMatrix(data_ground[cam][policy_column_name].to_numpy() , data_policy_temp[policy_percentage][policy_column_name].to_numpy() )\n",
    "            #ic(Confusion_Matrix.overall_stat)\n",
    "\n",
    "            # Errors\n",
    "            #ae_all = results[\"all_absolute-errors\"]\n",
    "            #difference_all = results[\"all_difference\"]\n",
    "            #ae_all_exploitation = results_exploitation[\"all_absolute-errors\"]\n",
    "            #difference_all_exploitation = results_exploitation[\"all_difference\"]\n",
    "            #ic(difference_all.index[103])\n",
    "            #ic(np.where(difference_all < 0))\n",
    "            #ic(difference_all.describe())\n",
    "            #for index, value in enumerate(difference_all):\n",
    "             #   print (difference_all[index], value)\n",
    "\n",
    "\n",
    "            # Absolute errors, group by date \n",
    "            #df_error_policy = pd.DataFrame({'timestamp':ae_all.index, 'errors':ae_all.values})\n",
    "            #df_error_policy = df_error_policy.groupby(by=df_error_policy['timestamp'].dt.date).agg({\"errors\":\"mean\", \"timestamp\":\"count\"})\n",
    "            #dae_policy[policy_percentage] =   df_error_policy[\"errors\"].mean()\n",
    "           \n",
    "\n",
    "\n",
    "    \n",
    "    cam_markers = {\n",
    "        \"jervskogen_1\": \"^\",\n",
    "        \"jervskogen_2\": \"v\",\n",
    "        \"nilsbyen_2\":   \"*\",\n",
    "        \"nilsbyen_3\":   \"8\",\n",
    "        \"skistua\":      \".\",\n",
    "        \"ronningen_1\":  \"S\"\n",
    "    }\n",
    "    cam_lines = {\n",
    "        \"jervskogen_1\": \"dashed\",\n",
    "        \"jervskogen_2\": \"solid\",\n",
    "        \"nilsbyen_2\":   \"dashed\",\n",
    "        \"nilsbyen_3\":   \"solid\",\n",
    "        \"skistua\":      \"solid\",\n",
    "        \"ronningen_1\":  \"S\"\n",
    "    }\n",
    "    \n",
    "    # Here we store all percentages error mean in dict\n",
    "    mean_dae_cams_policy_random[cam] = dae_policy_random.values()\n",
    "    mean_dae_cams_policy_learned[cam] = dae_policy_learned.values()\n",
    "\n",
    "  \n",
    "   \n",
    "# Mean of errors \n",
    "plt.close()\n",
    "x = np.arange(len(dae_policy_random))\n",
    "for cam in cams:    \n",
    "    plt.ylabel ('Missed no. of persons (%)') #Mean of Errors (Persons)\n",
    "    plt.xlabel ('Percentage of tiles send (%)')\n",
    "    random = plt.plot(x, mean_dae_cams_policy_random[cam],  color = \"black\", marker = cam_markers[cam],linestyle= \"dashed\", linewidth=1, alpha = 0.8, mfc='none', label = cam + \"R\")\n",
    "    learned = plt.plot(x, mean_dae_cams_policy_learned[cam],  color = \"black\", marker = cam_markers[cam],linestyle= \"solid\", linewidth=1, alpha = 0.8, mfc='none', label = cam)\n",
    "\n",
    "    plt.xticks (ticks = x, labels = dae_policy_learned.keys(), rotation = 0)\n",
    "\n",
    "#plt.title(\"All Cams\")\n",
    "#_ = plt.legend()\n",
    "\n",
    "# manually define a new patch \n",
    "#patch1 = mpatches.Patch(linestyle= \"solid\",linewidth=1.0, facecolor='none', fill=False, label='Random Policy')\n",
    "#patch2 = mpatches.Patch(linestyle= \"dashed\", label='Learned Policy')\n",
    "\n",
    "\n",
    "patch1 = Line2D([0], [0], marker='o', color='w', label='Circle',\n",
    "                        markerfacecolor='r', markersize=15)\n",
    "\n",
    "#patch2 = Line2D([0], [0], linestyle = \"solid\", linewidth=1.0, label='Random Policy' )\n",
    "#handles=patch1\n",
    "custom_policy = [Line2D([0], [0],  linestyle = \"dashed\",color=\"black\",  lw=1),\n",
    "                Line2D([0], [0],  linestyle = \"solid\", color=\"black\", lw=1)]\n",
    "#linestyle = \"none\"\n",
    "custom_cams = [Line2D([0], [0],  marker = cam_markers[\"jervskogen_1\"],  mfc='none', color=\"black\"),\n",
    "               Line2D([0], [0],  marker = cam_markers[\"jervskogen_2\"], mfc='none', color=\"black\"),\n",
    "               Line2D([0], [0],  marker = cam_markers[\"nilsbyen_2\"], mfc='none', color=\"black\"),\n",
    "               Line2D([0], [0],  marker = cam_markers[\"nilsbyen_3\"], mfc='none', color=\"black\"),\n",
    "               Line2D([0], [0],  marker = cam_markers[\"skistua\"], mfc='none', color=\"black\")]\n",
    "\n",
    "leg1 = plt.legend(custom_policy, ['Random', 'Learned'], ncol=1, loc='upper center',title= \"Policy\",  fancybox=False, shadow=False, frameon=False, bbox_to_anchor=(0.44, 1)) #(0.87, 1) cohen kappa\n",
    "leg2 = plt.legend(custom_cams, ['Jervskogen 1', 'Jervskogen 2', 'Nilsbyen 2', 'Nilsbyen 3', 'Skistua'], title= \"Locations\", loc='upper right', fancybox=False, shadow=False, frameon=False, bbox_to_anchor=(0.33, 1)) #(0.33, 0.5) cohen kappa\n",
    "# Manually add the first legend back\n",
    "plt.gca().add_artist(leg1)\n",
    "plt.savefig(database_path + 'local/' + exp_name_folder_learned + '/' + 'figures/' 'all_cams_difference_results' + '.pdf', dpi=300, format='pdf', bbox_inches='tight')\n",
    "plt.show()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181cfa8-5f6a-4026-b6a7-25a91a53721e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
